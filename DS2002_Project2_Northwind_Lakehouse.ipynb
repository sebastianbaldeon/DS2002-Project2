{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-2002 Data Project 2: Northwind Data Lakehouse\n",
    "**Student**: Sebastian Baldeon\n",
    "\n",
    "## Project Overview\n",
    "This project demonstrates building a dimensional Data Lakehouse using the Medallion Architecture (Bronze → Silver → Gold) with PySpark Structured Streaming. The solution integrates data from multiple sources:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section I: Setup and Configuration\n",
    "### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\ds2002\\.venv\\Lib\\site-packages\\pyspark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "import config.py as config\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Configure Connection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MYSQL_USER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# MySQL Configuration (Local OLTP Source)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m mysql_args = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhost_name\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33m127.0.0.1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33m3306\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdb_name\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mnorthwind\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconn_props\u001b[39m\u001b[33m\"\u001b[39m : {\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m : \u001b[43mMYSQL_USER\u001b[49m,\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpassword\u001b[39m\u001b[33m\"\u001b[39m : MYSQL_PASSWORD,\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdriver\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mcom.mysql.cj.jdbc.Driver\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m     }\n\u001b[32m     13\u001b[39m }\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# MongoDB Atlas Configuration (NoSQL Cloud Source)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     18\u001b[39m mongodb_args = {\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster_location\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33matlas\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muser_name\u001b[39m\u001b[33m\"\u001b[39m : MONGODB_USER,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnull_column_threshold\u001b[39m\u001b[33m\"\u001b[39m : \u001b[32m0.5\u001b[39m\n\u001b[32m     27\u001b[39m }\n",
      "\u001b[31mNameError\u001b[39m: name 'MYSQL_USER' is not defined"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# MySQL Configuration (Local OLTP Source)\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"127.0.0.1\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"northwind\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : MYSQL_USER,\n",
    "        \"password\" : MYSQL_PASSWORD,\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# MongoDB Atlas Configuration (NoSQL Cloud Source)\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"cluster_location\" : \"atlas\",\n",
    "    \"user_name\" : MONGODB_USER,\n",
    "    \"password\" : MONGODB_PASSWORD,\n",
    "    \"cluster_name\" : \"ds2002-lab6\",\n",
    "    \"cluster_subnet\" : \"fsfchfq\",\n",
    "    \"db_name\" : \"northwind_project2\",\n",
    "    \"collection\" : \"\",\n",
    "    \"null_column_threshold\" : 0.5\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Directory Structure for Streaming Data and Output\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.getcwd()\n",
    "streaming_data_dir = os.path.join(base_dir, 'streaming_data')\n",
    "\n",
    "# Data Lakehouse Output Directories\n",
    "dest_database = \"northwind_project2_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "# Bronze, Silver, Gold paths for fact_orders\n",
    "orders_output_bronze = os.path.join(database_dir, 'fact_orders', 'bronze')\n",
    "orders_output_silver = os.path.join(database_dir, 'fact_orders', 'silver')\n",
    "orders_output_gold = os.path.join(database_dir, 'fact_orders', 'gold')\n",
    "\n",
    "print(\"Configuration complete!\")\n",
    "print(f\"Streaming data directory: {streaming_data_dir}\")\n",
    "print(f\"Output database directory: {database_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "def get_mongo_uri(**args):\n",
    "    \"\"\"Generate MongoDB Atlas connection URI\"\"\"\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    \"\"\"Create Spark configuration with all necessary settings\"\"\"\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mysql_dataframe(spark_session, sql_query: str, **args):\n",
    "    \"\"\"Read data from MySQL using JDBC\"\"\"\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    \"\"\"Read data from MongoDB Atlas\"\"\"\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    # Drop the '_id' index column\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    \"\"\"Remove directory tree for clean restart\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    \"\"\"Wait for streaming query to process batches\"\"\"\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "    print(f\"Stream has processed {len(query.recentProgress)} batches\")\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0. Clean Up Previous Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'c:\\Users\\sebas\\ds2002\\final_project\\spark-warehouse\\northwind_project2_dlh.db' removed successfully.\n"
     ]
    }
   ],
   "source": [
    "result = remove_directory_tree(database_dir)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0. Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark session created successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sebas.myfiosgateway.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DS-2002 Project 2 - Northwind Data Lakehouse</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29332620f90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark configuration arguments\n",
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "mongo_uri = get_mongo_uri(**mongodb_args)\n",
    "\n",
    "# MySQL JAR path - UPDATED TO CORRECT LOCATION\n",
    "mysql_jar_path = r\"C:\\Users\\sebas\\DS-2002\\04-PySpark\\mysql-connector-j-9.1.0\\mysql-connector-j-9.1.0.jar\"\n",
    "\n",
    "sparkConf_args = {\n",
    "    \"app_name\" : \"DS-2002 Project 2 - Northwind Data Lakehouse\",\n",
    "    \"worker_threads\" : worker_threads,\n",
    "    \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "    \"mongo_uri\" : mongo_uri,\n",
    "    \"spark_jars\" : mysql_jar_path,\n",
    "    \"database_dir\" : sql_warehouse_dir\n",
    "}\n",
    "\n",
    "# Create Spark configuration\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\" Spark session created successfully!\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0. Create Data Lakehouse Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Database 'northwind_project2_dlh' created!\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "COMMENT 'DS-2002 Project 2 - Northwind Data Lakehouse'\n",
    "WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Final Project')\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql_create_db)\n",
    "print(f\" Database '{dest_database}' created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II: Load Dimension Tables\n",
    "### 1.0. Load Dimensions from MongoDB Atlas (NoSQL Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 29 customers from MongoDB Atlas\n",
      "+-------+------------+-------------+\n",
      "|country|customer_key|customer_name|\n",
      "+-------+------------+-------------+\n",
      "|    USA|          12|    Company L|\n",
      "|    USA|          16|    Company P|\n",
      "|    USA|           2|    Company B|\n",
      "|    USA|          21|    Company U|\n",
      "|    USA|          24|    Company X|\n",
      "+-------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dim_customers from MongoDB\n",
    "mongodb_args['collection'] = 'dim_customers'\n",
    "dim_customers = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "\n",
    "print(f\" Loaded {dim_customers.count()} customers from MongoDB Atlas\")\n",
    "dim_customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 45 products from MongoDB Atlas\n",
      "+-----------+--------------------+----------+\n",
      "|product_key|        product_name|unit_price|\n",
      "+-----------+--------------------+----------+\n",
      "|         14|Northwind Traders...|     23.25|\n",
      "|         19|Northwind Traders...|       9.2|\n",
      "|         56|Northwind Traders...|      38.0|\n",
      "|         83|Northwind Traders...|       1.8|\n",
      "|         90|Northwind Traders...|       1.8|\n",
      "+-----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dim_products from MongoDB\n",
    "mongodb_args['collection'] = 'dim_products'\n",
    "dim_products = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "\n",
    "print(f\" Loaded {dim_products.count()} products from MongoDB Atlas\")\n",
    "dim_products.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Load Dimensions from MySQL (Relational Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 9 employees from MySQL\n",
      "+---+-----------------+---------+----------+--------------------+--------------------+--------------+-------------+------------+-------------+--------------+--------+--------------+---------------+--------------+--------------------+--------------------+-----------+\n",
      "| id|          company|last_name|first_name|       email_address|           job_title|business_phone|   home_phone|mobile_phone|   fax_number|       address|    city|state_province|zip_postal_code|country_region|            web_page|               notes|attachments|\n",
      "+---+-----------------+---------+----------+--------------------+--------------------+--------------+-------------+------------+-------------+--------------+--------+--------------+---------------+--------------+--------------------+--------------------+-----------+\n",
      "|  1|Northwind Traders|Freehafer|     Nancy|nancy@northwindtr...|Sales Representative| (123)555-0100|(123)555-0102|        NULL|(123)555-0103|123 1st Avenue| Seattle|            WA|          99999|           USA|#http://northwind...|                NULL|         []|\n",
      "|  2|Northwind Traders|  Cencini|    Andrew|andrew@northwindt...|Vice President, S...| (123)555-0100|(123)555-0102|        NULL|(123)555-0103|123 2nd Avenue|Bellevue|            WA|          99999|           USA|http://northwindt...|Joined the compan...|         []|\n",
      "|  3|Northwind Traders|    Kotas|       Jan|jan@northwindtrad...|Sales Representative| (123)555-0100|(123)555-0102|        NULL|(123)555-0103|123 3rd Avenue| Redmond|            WA|          99999|           USA|http://northwindt...|Was hired as a sa...|         []|\n",
      "|  4|Northwind Traders|Sergienko|    Mariya|mariya@northwindt...|Sales Representative| (123)555-0100|(123)555-0102|        NULL|(123)555-0103|123 4th Avenue|Kirkland|            WA|          99999|           USA|http://northwindt...|                NULL|         []|\n",
      "|  5|Northwind Traders|   Thorpe|    Steven|steven@northwindt...|       Sales Manager| (123)555-0100|(123)555-0102|        NULL|(123)555-0103|123 5th Avenue| Seattle|            WA|          99999|           USA|http://northwindt...|Joined the compan...|         []|\n",
      "+---+-----------------+---------+----------+--------------------+--------------------+--------------+-------------+------------+-------------+--------------+--------+--------------+---------------+--------------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dim_employees from local MySQL\n",
    "sql_employees = \"SELECT * FROM employees\"\n",
    "dim_employees = get_mysql_dataframe(spark, sql_employees, **mysql_args)\n",
    "\n",
    "print(f\" Loaded {dim_employees.count()} employees from MySQL\")\n",
    "dim_employees.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 3 shippers from MySQL\n",
      "+---+------------------+---------+----------+-------------+---------+--------------+----------+------------+----------+--------------+-------+--------------+---------------+--------------+--------+-----+-----------+\n",
      "| id|           company|last_name|first_name|email_address|job_title|business_phone|home_phone|mobile_phone|fax_number|       address|   city|state_province|zip_postal_code|country_region|web_page|notes|attachments|\n",
      "+---+------------------+---------+----------+-------------+---------+--------------+----------+------------+----------+--------------+-------+--------------+---------------+--------------+--------+-----+-----------+\n",
      "|  1|Shipping Company A|     NULL|      NULL|         NULL|     NULL|          NULL|      NULL|        NULL|      NULL|123 Any Street|Memphis|            TN|          99999|           USA|    NULL| NULL|         []|\n",
      "|  2|Shipping Company B|     NULL|      NULL|         NULL|     NULL|          NULL|      NULL|        NULL|      NULL|123 Any Street|Memphis|            TN|          99999|           USA|    NULL| NULL|         []|\n",
      "|  3|Shipping Company C|     NULL|      NULL|         NULL|     NULL|          NULL|      NULL|        NULL|      NULL|123 Any Street|Memphis|            TN|          99999|           USA|    NULL| NULL|         []|\n",
      "+---+------------------+---------+----------+-------------+---------+--------------+----------+------------+----------+--------------+-------+--------------+---------------+--------------+--------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dim_shippers from local MySQL\n",
    "sql_shippers = \"SELECT * FROM shippers\"\n",
    "dim_shippers = get_mysql_dataframe(spark, sql_shippers, **mysql_args)\n",
    "\n",
    "print(f\" Loaded {dim_shippers.count()} shippers from MySQL\")\n",
    "dim_shippers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0. Create Date Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated date dimension with 365 dates\n",
      "+----------+----+-----+---+-----------+-----------+------------+-------+---------+----------+\n",
      "| full_date|year|month|day|day_of_week|day_of_year|week_of_year|quarter| day_name|month_name|\n",
      "+----------+----+-----+---+-----------+-----------+------------+-------+---------+----------+\n",
      "|2006-01-01|2006|    1|  1|          1|          1|          52|      1|   Sunday|   January|\n",
      "|2006-01-02|2006|    1|  2|          2|          2|           1|      1|   Monday|   January|\n",
      "|2006-01-03|2006|    1|  3|          3|          3|           1|      1|  Tuesday|   January|\n",
      "|2006-01-04|2006|    1|  4|          4|          4|           1|      1|Wednesday|   January|\n",
      "|2006-01-05|2006|    1|  5|          5|          5|           1|      1| Thursday|   January|\n",
      "+----------+----+-----+---+-----------+-----------+------------+-------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create date dimension from 2006-01-01 to 2006-12-31\n",
    "date_range_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        sequence(to_date('2006-01-01'), to_date('2006-12-31'), interval 1 day) as date\n",
    "\"\"\")\n",
    "\n",
    "dim_date = date_range_df.select(explode(col(\"date\")).alias(\"full_date\")) \\\n",
    "    .select(\n",
    "        col(\"full_date\"),\n",
    "        year(\"full_date\").alias(\"year\"),\n",
    "        month(\"full_date\").alias(\"month\"),\n",
    "        dayofmonth(\"full_date\").alias(\"day\"),\n",
    "        dayofweek(\"full_date\").alias(\"day_of_week\"),\n",
    "        dayofyear(\"full_date\").alias(\"day_of_year\"),\n",
    "        weekofyear(\"full_date\").alias(\"week_of_year\"),\n",
    "        quarter(\"full_date\").alias(\"quarter\"),\n",
    "        date_format(\"full_date\", \"EEEE\").alias(\"day_name\"),\n",
    "        date_format(\"full_date\", \"MMMM\").alias(\"month_name\")\n",
    "    )\n",
    "\n",
    "print(f\" Generated date dimension with {dim_date.count()} dates\")\n",
    "dim_date.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section III: Streaming Data Integration (Bronze → Silver → Gold)\n",
    "### Bronze Layer: Ingest Streaming Order Details from JSON Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0. Define Schema for Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Schema defined for streaming order details\n"
     ]
    }
   ],
   "source": [
    "# Define schema for incoming order details stream\n",
    "order_details_schema = StructType([\n",
    "    StructField(\"order_detail_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", DoubleType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"discount\", DoubleType(), True),\n",
    "    StructField(\"status_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"shipper_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "print(\" Schema defined for streaming order details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0. Convert JSON Files to Newline-Delimited Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08025de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating streaming data files from MySQL...\n",
      "✓ Extracted 58 order detail records from MySQL\n",
      "✓ Created batch1 with 19 records\n",
      "✓ Created batch2 with 19 records\n",
      "✓ Created batch3 with 20 records\n",
      "\n",
      "✓ All streaming data files created in: c:\\Users\\sebas\\ds2002\\final_project\\streaming_data\n",
      "Files: ['order_details_batch1.json', 'order_details_batch2.json', 'order_details_batch3.json', 'test.json']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE STREAMING DATA FILES FROM MySQL\n",
    "# ============================================================================\n",
    "import json\n",
    "import mysql.connector\n",
    "from decimal import Decimal\n",
    "\n",
    "print(\"Creating streaming data files from MySQL...\")\n",
    "\n",
    "# Helper function to convert Decimal to float\n",
    "def decimal_to_float(obj):\n",
    "    if isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    raise TypeError\n",
    "\n",
    "# Connect to MySQL\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"sebastian\",\n",
    "    password=\"SebasBal5*\",\n",
    "    database=\"northwind\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor(dictionary=True)\n",
    "\n",
    "# Get all order details with order information\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        od.id as order_detail_id,\n",
    "        o.id as order_id,\n",
    "        od.product_id,\n",
    "        od.quantity,\n",
    "        od.unit_price,\n",
    "        od.discount,\n",
    "        od.status_id,\n",
    "        DATE_FORMAT(o.order_date, '%Y-%m-%d') as order_date,\n",
    "        o.customer_id,\n",
    "        o.employee_id,\n",
    "        o.shipper_id\n",
    "    FROM order_details od\n",
    "    JOIN orders o ON od.order_id = o.id\n",
    "    ORDER BY o.order_date\n",
    "\"\"\")\n",
    "\n",
    "all_records = cursor.fetchall()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\" Extracted {len(all_records)} order detail records from MySQL\")\n",
    "\n",
    "# Split into 3 batches for streaming simulation\n",
    "total = len(all_records)\n",
    "batch1 = all_records[:total//3]\n",
    "batch2 = all_records[total//3:(total//3)*2]\n",
    "batch3 = all_records[(total//3)*2:]\n",
    "\n",
    "# Create streaming_data directory if it doesn't exist\n",
    "os.makedirs(streaming_data_dir, exist_ok=True)\n",
    "\n",
    "# Write each batch to a JSON file with Decimal conversion\n",
    "for i, batch in enumerate([batch1, batch2, batch3], 1):\n",
    "    file_path = os.path.join(streaming_data_dir, f'order_details_batch{i}.json')\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(batch, f, indent=2, default=decimal_to_float)\n",
    "    print(f\" Created batch{i} with {len(batch)} records\")\n",
    "\n",
    "print(f\"\\n All streaming data files created in: {streaming_data_dir}\")\n",
    "print(f\"Files: {os.listdir(streaming_data_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted batch1 (19 records)\n",
      "✓ Converted batch2 (19 records)\n",
      "✓ Converted batch3 (20 records)\n",
      "\n",
      "✓ All files converted to newline-delimited JSON!\n"
     ]
    }
   ],
   "source": [
    "# Convert JSON files from array format to newline-delimited format\n",
    "# Spark streaming requires one JSON object per line\n",
    "\n",
    "for i in range(1, 4):\n",
    "    file_path = os.path.join(streaming_data_dir, f'order_details_batch{i}.json')\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"  File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Read array-format file\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Overwrite with newline-delimited format\n",
    "    with open(file_path, 'w') as f:\n",
    "        for record in data:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "    \n",
    "    print(f\" Converted batch{i} ({len(data)} records)\")\n",
    "\n",
    "print(\"\\n All files converted to newline-delimited JSON!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0. Configure Bronze Streaming Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bronze stream configured\n",
      "Schema:\n",
      "root\n",
      " |-- order_detail_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      " |-- status_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- shipper_id: integer (nullable = true)\n",
      " |-- ingestion_timestamp: timestamp (nullable = false)\n",
      " |-- source_file: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read streaming data from JSON files (AutoLoader pattern)\n",
    "bronze_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(order_details_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(streaming_data_dir + \"/order_details_*.json\")\n",
    "\n",
    "# Add metadata columns\n",
    "bronze_stream = bronze_stream \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "print(\" Bronze stream configured\")\n",
    "print(\"Schema:\")\n",
    "bronze_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0. Write Bronze Stream to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bronze stream started - processing JSON batches...\n",
      "Waiting for batches to process...\n",
      "Stream has processed 3 batches\n",
      "✓ Bronze layer complete!\n"
     ]
    }
   ],
   "source": [
    "# Write bronze stream to Parquet format\n",
    "bronze_query = bronze_stream.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"{orders_output_bronze}_checkpoint\") \\\n",
    "    .start(orders_output_bronze)\n",
    "\n",
    "print(\" Bronze stream started - processing JSON batches...\")\n",
    "print(\"Waiting for batches to process...\")\n",
    "\n",
    "# Wait for all 3 batches to process\n",
    "wait_until_stream_is_ready(bronze_query, min_batches=3)\n",
    "\n",
    "# Stop the bronze stream\n",
    "bronze_query.stop()\n",
    "print(\" Bronze layer complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0. Verify Bronze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bronze table contains 58 order detail records\n",
      "+---------------+--------+----------+--------+----------+--------+---------+----------+-----------+-----------+----------+--------------------+--------------------+\n",
      "|order_detail_id|order_id|product_id|quantity|unit_price|discount|status_id|order_date|customer_id|employee_id|shipper_id| ingestion_timestamp|         source_file|\n",
      "+---------------+--------+----------+--------+----------+--------+---------+----------+-----------+-----------+----------+--------------------+--------------------+\n",
      "|             27|      30|        34|   100.0|      14.0|     0.0|        2|2006-01-15|         27|          9|         2|2025-12-15 19:30:...|file:///c:/Users/...|\n",
      "|             28|      30|        80|    30.0|       3.5|     0.0|        2|2006-01-15|         27|          9|         2|2025-12-15 19:30:...|file:///c:/Users/...|\n",
      "|             29|      31|         7|    10.0|      30.0|     0.0|        2|2006-01-20|          4|          3|         1|2025-12-15 19:30:...|file:///c:/Users/...|\n",
      "|             30|      31|        51|    10.0|      53.0|     0.0|        2|2006-01-20|          4|          3|         1|2025-12-15 19:30:...|file:///c:/Users/...|\n",
      "|             31|      31|        80|    10.0|       3.5|     0.0|        2|2006-01-20|          4|          3|         1|2025-12-15 19:30:...|file:///c:/Users/...|\n",
      "|             32|      32|         1|    15.0|      18.0|     0.0|        2|2006-01-22|         12|          4|         2|2025-12-15 19:30:...|file:///c:/Users/...|\n",
      "|             33|      32|        43|    20.0|      46.0|     0.0|        2|2006-01-22|         12|          4|         2|2025-12-15 19:30:...|file:///c:/Users/...|\n",
      "|             34|      33|        19|    30.0|       9.2|     0.0|        2|2006-01-30|          8|          6|         3|2025-12-15 19:30:...|file:///c:/Users/...|\n",
      "|             35|      34|        19|    20.0|       9.2|     0.0|        2|2006-02-06|          4|          9|         3|2025-12-15 19:30:...|file:///c:/Users/...|\n",
      "|             36|      35|        48|    10.0|     12.75|     0.0|        2|2006-02-10|         29|          3|         2|2025-12-15 19:30:...|file:///c:/Users/...|\n",
      "+---------------+--------+----------+--------+----------+--------+---------+----------+-----------+-----------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify bronze data\n",
    "bronze_df = spark.read.format(\"parquet\").load(orders_output_bronze)\n",
    "print(f\" Bronze table contains {bronze_df.count()} order detail records\")\n",
    "bronze_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver Layer: Join Streaming Data with Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Silver stream configured with dimension joins\n",
      "root\n",
      " |-- order_detail_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      " |-- status_id: integer (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- shipper_id: integer (nullable = true)\n",
      " |-- ingestion_timestamp: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      " |-- customer_key: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_country: string (nullable = true)\n",
      " |-- product_key: integer (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- list_price: double (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- shipper_name: string (nullable = true)\n",
      " |-- line_total: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from bronze as a stream\n",
    "silver_stream = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(orders_output_bronze)\n",
    "\n",
    "# Convert order_date to proper date type\n",
    "silver_stream = silver_stream.withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "\n",
    "# Join with dim_customers (MongoDB) - NO CITY COLUMN\n",
    "silver_stream = silver_stream \\\n",
    "    .join(dim_customers.select(\"customer_key\", \"customer_name\", \"country\"), \n",
    "          silver_stream.customer_id == dim_customers.customer_key, \"left\") \\\n",
    "    .withColumnRenamed(\"country\", \"customer_country\")\n",
    "\n",
    "# Join with dim_products (MongoDB)\n",
    "silver_stream = silver_stream \\\n",
    "    .join(dim_products.select(\"product_key\", \"product_name\", col(\"unit_price\").alias(\"list_price\")), \n",
    "          silver_stream.product_id == dim_products.product_key, \"left\")\n",
    "\n",
    "# Join with dim_employees (MySQL)\n",
    "silver_stream = silver_stream \\\n",
    "    .join(\n",
    "        dim_employees.select(\n",
    "            col(\"id\").alias(\"emp_id\"),\n",
    "            concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")).alias(\"employee_name\")\n",
    "        ), \n",
    "        silver_stream.employee_id == col(\"emp_id\"), \n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .drop(\"emp_id\")\n",
    "\n",
    "# Join with dim_shippers (MySQL)\n",
    "silver_stream = silver_stream \\\n",
    "    .join(\n",
    "        dim_shippers.select(\n",
    "            col(\"id\").alias(\"ship_id\"),\n",
    "            col(\"company\").alias(\"shipper_name\")\n",
    "        ), \n",
    "        silver_stream.shipper_id == col(\"ship_id\"), \n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .drop(\"ship_id\")\n",
    "\n",
    "# Calculate line_total\n",
    "silver_stream = silver_stream.withColumn(\n",
    "    \"line_total\", \n",
    "    col(\"quantity\") * col(\"unit_price\") * (1 - col(\"discount\"))\n",
    ")\n",
    "\n",
    "print(\" Silver stream configured with dimension joins\")\n",
    "silver_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Silver Stream to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Silver stream started - joining with dimensions...\n",
      "Waiting for processing...\n",
      "Stream has processed 3 batches\n",
      "✓ Silver layer complete!\n"
     ]
    }
   ],
   "source": [
    "# Write silver stream to Parquet\n",
    "silver_query = silver_stream.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"{orders_output_silver}_checkpoint\") \\\n",
    "    .start(orders_output_silver)\n",
    "\n",
    "print(\" Silver stream started - joining with dimensions...\")\n",
    "print(\"Waiting for processing...\")\n",
    "\n",
    "# Wait for processing\n",
    "wait_until_stream_is_ready(silver_query, min_batches=3)\n",
    "\n",
    "# Stop the silver stream\n",
    "silver_query.stop()\n",
    "print(\" Silver layer complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Silver Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Silver table contains 58 enriched records\n",
      "+---------------+--------+----------+--------+----------+--------+---------+----------+-----------+-----------+----------+--------------------+--------------------+------------+-------------+----------------+-----------+--------------------+----------+-------------------+------------------+----------+\n",
      "|order_detail_id|order_id|product_id|quantity|unit_price|discount|status_id|order_date|customer_id|employee_id|shipper_id| ingestion_timestamp|         source_file|customer_key|customer_name|customer_country|product_key|        product_name|list_price|      employee_name|      shipper_name|line_total|\n",
      "+---------------+--------+----------+--------+----------+--------+---------+----------+-----------+-----------+----------+--------------------+--------------------+------------+-------------+----------------+-----------+--------------------+----------+-------------------+------------------+----------+\n",
      "|             79|      69|        80|    15.0|       3.5|     0.0|        2|2006-05-24|         10|          1|         1|2025-12-15 19:30:...|file:///c:/Users/...|          10|    Company J|             USA|         80|Northwind Traders...|       3.5|    Nancy Freehafer|Shipping Company A|      52.5|\n",
      "|             43|      42|         6|    10.0|      25.0|     0.0|        2|2006-03-24|         10|          1|         1|2025-12-15 19:30:...|file:///c:/Users/...|          10|    Company J|             USA|          6|Northwind Traders...|      25.0|    Nancy Freehafer|Shipping Company A|     250.0|\n",
      "|             45|      42|        19|    10.0|       9.2|     0.0|        2|2006-03-24|         10|          1|         1|2025-12-15 19:30:...|file:///c:/Users/...|          10|    Company J|             USA|         19|Northwind Traders...|       9.2|    Nancy Freehafer|Shipping Company A|      92.0|\n",
      "|             44|      42|         4|    10.0|      22.0|     0.0|        2|2006-03-24|         10|          1|         1|2025-12-15 19:30:...|file:///c:/Users/...|          10|    Company J|             USA|          4|Northwind Traders...|      22.0|    Nancy Freehafer|Shipping Company A|     220.0|\n",
      "|             31|      31|        80|    10.0|       3.5|     0.0|        2|2006-01-20|          4|          3|         1|2025-12-15 19:30:...|file:///c:/Users/...|           4|    Company D|             USA|         80|Northwind Traders...|       3.5|          Jan Kotas|Shipping Company A|      35.0|\n",
      "|             84|      58|        20|    40.0|      81.0|     0.0|        2|2006-04-22|          4|          3|         1|2025-12-15 19:30:...|file:///c:/Users/...|           4|    Company D|             USA|         20|Northwind Traders...|      81.0|          Jan Kotas|Shipping Company A|    3240.0|\n",
      "|             30|      31|        51|    10.0|      53.0|     0.0|        2|2006-01-20|          4|          3|         1|2025-12-15 19:30:...|file:///c:/Users/...|           4|    Company D|             USA|         51|Northwind Traders...|      53.0|          Jan Kotas|Shipping Company A|     530.0|\n",
      "|             29|      31|         7|    10.0|      30.0|     0.0|        2|2006-01-20|          4|          3|         1|2025-12-15 19:30:...|file:///c:/Users/...|           4|    Company D|             USA|          7|Northwind Traders...|      30.0|          Jan Kotas|Shipping Company A|     300.0|\n",
      "|             85|      58|        52|    40.0|       7.0|     0.0|        2|2006-04-22|          4|          3|         1|2025-12-15 19:30:...|file:///c:/Users/...|           4|    Company D|             USA|         52|Northwind Traders...|       7.0|          Jan Kotas|Shipping Company A|     280.0|\n",
      "|             72|      76|         4|    30.0|      22.0|     0.0|        2|2006-06-05|         25|          9|         1|2025-12-15 19:30:...|file:///c:/Users/...|          25|    Company Y|             USA|          4|Northwind Traders...|      22.0|Anne Hellung-Larsen|Shipping Company A|     660.0|\n",
      "+---------------+--------+----------+--------+----------+--------+---------+----------+-----------+-----------+----------+--------------------+--------------------+------------+-------------+----------------+-----------+--------------------+----------+-------------------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify silver data\n",
    "silver_df = spark.read.format(\"parquet\").load(orders_output_silver)\n",
    "print(f\" Silver table contains {silver_df.count()} enriched records\")\n",
    "silver_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------------+--------------------+-------------------+------------------+----------+\n",
      "|order_id|customer_name|customer_country|        product_name|      employee_name|      shipper_name|line_total|\n",
      "+--------+-------------+----------------+--------------------+-------------------+------------------+----------+\n",
      "|      69|    Company J|             USA|Northwind Traders...|    Nancy Freehafer|Shipping Company A|      52.5|\n",
      "|      42|    Company J|             USA|Northwind Traders...|    Nancy Freehafer|Shipping Company A|     250.0|\n",
      "|      42|    Company J|             USA|Northwind Traders...|    Nancy Freehafer|Shipping Company A|      92.0|\n",
      "|      42|    Company J|             USA|Northwind Traders...|    Nancy Freehafer|Shipping Company A|     220.0|\n",
      "|      31|    Company D|             USA|Northwind Traders...|          Jan Kotas|Shipping Company A|      35.0|\n",
      "|      58|    Company D|             USA|Northwind Traders...|          Jan Kotas|Shipping Company A|    3240.0|\n",
      "|      31|    Company D|             USA|Northwind Traders...|          Jan Kotas|Shipping Company A|     530.0|\n",
      "|      31|    Company D|             USA|Northwind Traders...|          Jan Kotas|Shipping Company A|     300.0|\n",
      "|      58|    Company D|             USA|Northwind Traders...|          Jan Kotas|Shipping Company A|     280.0|\n",
      "|      76|    Company Y|             USA|Northwind Traders...|Anne Hellung-Larsen|Shipping Company A|     660.0|\n",
      "+--------+-------------+----------------+--------------------+-------------------+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show specific enriched columns\n",
    "silver_df.select(\"order_id\", \"customer_name\", \"customer_country\", \"product_name\", \"employee_name\", \"shipper_name\", \"line_total\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold Layer: Final Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gold layer complete!\n",
      "✓ Final fact_orders table contains 58 records\n",
      "+--------+--------+----------+------------+-------------+----------------+-----------+--------------------+------------+-------------------+-----------+------------------+--------+----------+--------+----------+\n",
      "|fact_key|order_id|order_date|customer_key|customer_name|customer_country|product_key|        product_name|employee_key|      employee_name|shipper_key|      shipper_name|quantity|unit_price|discount|line_total|\n",
      "+--------+--------+----------+------------+-------------+----------------+-----------+--------------------+------------+-------------------+-----------+------------------+--------+----------+--------+----------+\n",
      "|      79|      69|2006-05-24|          10|    Company J|             USA|         80|Northwind Traders...|           1|    Nancy Freehafer|          1|Shipping Company A|    15.0|       3.5|     0.0|      52.5|\n",
      "|      43|      42|2006-03-24|          10|    Company J|             USA|          6|Northwind Traders...|           1|    Nancy Freehafer|          1|Shipping Company A|    10.0|      25.0|     0.0|     250.0|\n",
      "|      45|      42|2006-03-24|          10|    Company J|             USA|         19|Northwind Traders...|           1|    Nancy Freehafer|          1|Shipping Company A|    10.0|       9.2|     0.0|      92.0|\n",
      "|      44|      42|2006-03-24|          10|    Company J|             USA|          4|Northwind Traders...|           1|    Nancy Freehafer|          1|Shipping Company A|    10.0|      22.0|     0.0|     220.0|\n",
      "|      31|      31|2006-01-20|           4|    Company D|             USA|         80|Northwind Traders...|           3|          Jan Kotas|          1|Shipping Company A|    10.0|       3.5|     0.0|      35.0|\n",
      "|      84|      58|2006-04-22|           4|    Company D|             USA|         20|Northwind Traders...|           3|          Jan Kotas|          1|Shipping Company A|    40.0|      81.0|     0.0|    3240.0|\n",
      "|      30|      31|2006-01-20|           4|    Company D|             USA|         51|Northwind Traders...|           3|          Jan Kotas|          1|Shipping Company A|    10.0|      53.0|     0.0|     530.0|\n",
      "|      29|      31|2006-01-20|           4|    Company D|             USA|          7|Northwind Traders...|           3|          Jan Kotas|          1|Shipping Company A|    10.0|      30.0|     0.0|     300.0|\n",
      "|      85|      58|2006-04-22|           4|    Company D|             USA|         52|Northwind Traders...|           3|          Jan Kotas|          1|Shipping Company A|    40.0|       7.0|     0.0|     280.0|\n",
      "|      72|      76|2006-06-05|          25|    Company Y|             USA|          4|Northwind Traders...|           9|Anne Hellung-Larsen|          1|Shipping Company A|    30.0|      22.0|     0.0|     660.0|\n",
      "+--------+--------+----------+------------+-------------+----------------+-----------+--------------------+------------+-------------------+-----------+------------------+--------+----------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read silver data (batch mode for final transformation)\n",
    "gold_df = spark.read.format(\"parquet\").load(orders_output_silver)\n",
    "\n",
    "# Select final fact table columns\n",
    "fact_orders = gold_df.select(\n",
    "    col(\"order_detail_id\").alias(\"fact_key\"),\n",
    "    col(\"order_id\"),\n",
    "    col(\"order_date\"),\n",
    "    col(\"customer_key\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"customer_country\"),\n",
    "    col(\"product_key\"),\n",
    "    col(\"product_name\"),\n",
    "    col(\"employee_id\").alias(\"employee_key\"),\n",
    "    col(\"employee_name\"),\n",
    "    col(\"shipper_id\").alias(\"shipper_key\"),\n",
    "    col(\"shipper_name\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\"unit_price\"),\n",
    "    col(\"discount\"),\n",
    "    col(\"line_total\")\n",
    ")\n",
    "\n",
    "# Write to Gold layer as Parquet\n",
    "fact_orders.write.format(\"parquet\").mode(\"overwrite\").save(orders_output_gold)\n",
    "\n",
    "print(\" Gold layer complete!\")\n",
    "print(f\" Final fact_orders table contains {fact_orders.count()} records\")\n",
    "fact_orders.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section IV: Business Analytics Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0. Total Sales by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sales by Country:\n",
      "+----------------+-----------+\n",
      "|customer_country|total_sales|\n",
      "+----------------+-----------+\n",
      "|             USA|    68137.0|\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load gold fact table\n",
    "fact_orders_df = spark.read.format(\"parquet\").load(orders_output_gold)\n",
    "\n",
    "# Total sales by country\n",
    "sales_by_country = fact_orders_df.groupBy(\"customer_country\") \\\n",
    "    .agg(sum(\"line_total\").alias(\"total_sales\")) \\\n",
    "    .orderBy(desc(\"total_sales\"))\n",
    "\n",
    "print(\"Total Sales by Country:\")\n",
    "sales_by_country.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Top Performing Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Performing Employees:\n",
      "+-------------------+-------------+----------+\n",
      "|      employee_name|total_revenue|num_orders|\n",
      "+-------------------+-------------+----------+\n",
      "|    Nancy Freehafer|     22255.25|        17|\n",
      "|Anne Hellung-Larsen|     19974.25|        10|\n",
      "|    Michael Neipper|       6378.0|         4|\n",
      "|   Mariya Sergienko|       6278.0|        10|\n",
      "|          Jan Kotas|       5787.5|         7|\n",
      "|        Robert Zare|       3786.5|         3|\n",
      "|     Andrew Cencini|       2997.5|         6|\n",
      "|     Laura Giussani|        680.0|         1|\n",
      "+-------------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top performing employees by revenue\n",
    "top_employees = fact_orders_df.groupBy(\"employee_name\") \\\n",
    "    .agg(\n",
    "        sum(\"line_total\").alias(\"total_revenue\"),\n",
    "        count(\"*\").alias(\"num_orders\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "print(\"Top Performing Employees:\")\n",
    "top_employees.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0. Product Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Products by Revenue:\n",
      "+--------------------+-------------+--------------+\n",
      "|        product_name|total_revenue|total_quantity|\n",
      "+--------------------+-------------+--------------+\n",
      "|Northwind Traders...|      29900.0|         650.0|\n",
      "|Northwind Traders...|       6818.0|         487.0|\n",
      "|Northwind Traders...|       3240.0|          40.0|\n",
      "|Northwind Traders...|       3132.0|          90.0|\n",
      "|Northwind Traders...|       2798.5|         290.0|\n",
      "|Northwind Traders...|       2600.0|          65.0|\n",
      "|Northwind Traders...|       2550.0|         200.0|\n",
      "|Northwind Traders...|       2500.0|         100.0|\n",
      "|Northwind Traders...|       2208.0|         120.0|\n",
      "|Northwind Traders...|       2120.0|          40.0|\n",
      "+--------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 products by revenue\n",
    "top_products = fact_orders_df.groupBy(\"product_name\") \\\n",
    "    .agg(\n",
    "        sum(\"line_total\").alias(\"total_revenue\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_revenue\")) \\\n",
    "    .limit(10)\n",
    "\n",
    "print(\"Top 10 Products by Revenue:\")\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0. Monthly Sales Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Sales Trends:\n",
      "+----+-----+-----------+----------+\n",
      "|year|month|total_sales|num_orders|\n",
      "+----+-----+-----------+----------+\n",
      "|2006|    1|     3836.0|         8|\n",
      "|2006|    2|     2241.5|         3|\n",
      "|2006|    3|   32609.25|        13|\n",
      "|2006|    4|   19355.25|        21|\n",
      "|2006|    5|     1788.5|         4|\n",
      "|2006|    6|     8306.5|         9|\n",
      "+----+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monthly sales trends\n",
    "monthly_sales = fact_orders_df \\\n",
    "    .withColumn(\"year\", year(\"order_date\")) \\\n",
    "    .withColumn(\"month\", month(\"order_date\")) \\\n",
    "    .groupBy(\"year\", \"month\") \\\n",
    "    .agg(\n",
    "        sum(\"line_total\").alias(\"total_sales\"),\n",
    "        count(\"*\").alias(\"num_orders\")\n",
    "    ) \\\n",
    "    .orderBy(\"year\", \"month\")\n",
    "\n",
    "print(\"Monthly Sales Trends:\")\n",
    "monthly_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0. Customer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fact_orders_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Top 10 customers by total spending\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m top_customers = \u001b[43mfact_orders_df\u001b[49m.groupBy(\u001b[33m\"\u001b[39m\u001b[33mcustomer_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcustomer_city\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcustomer_country\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      3\u001b[39m     .agg(\n\u001b[32m      4\u001b[39m         \u001b[38;5;28msum\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mline_total\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mtotal_spent\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m         count(\u001b[33m\"\u001b[39m\u001b[33morder_id\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mnum_orders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     ) \\\n\u001b[32m      7\u001b[39m     .orderBy(desc(\u001b[33m\"\u001b[39m\u001b[33mtotal_spent\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m      8\u001b[39m     .limit(\u001b[32m10\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTop 10 Customers by Total Spending:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m top_customers.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'fact_orders_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Top 10 customers by total spending\n",
    "top_customers = fact_orders_df.groupBy(\"customer_name\", \"customer_city\", \"customer_country\") \\\n",
    "    .agg(\n",
    "        sum(\"line_total\").alias(\"total_spent\"),\n",
    "        count(\"order_id\").alias(\"num_orders\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_spent\")) \\\n",
    "    .limit(10)\n",
    "\n",
    "print(\"Top 10 Customers by Total Spending:\")\n",
    "top_customers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section V: Project Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DS-2002 Project 2: Northwind Data Lakehouse - COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Multi-Source Data Integration:\")\n",
    "print(f\"  • MongoDB Atlas (NoSQL): {dim_customers.count()} customers, {dim_products.count()} products\")\n",
    "print(f\"  • MySQL (Relational): {dim_employees.count()} employees, {dim_shippers.count()} shippers\")\n",
    "print(f\"  • File System (Streaming): {bronze_df.count()} order details from 3 JSON batches\")\n",
    "print(\"\\n✓ Medallion Architecture Implemented:\")\n",
    "print(f\"  • Bronze Layer: {bronze_df.count()} raw records\")\n",
    "print(f\"  • Silver Layer: {silver_df.count()} enriched records with dimension joins\")\n",
    "print(f\"  • Gold Layer: {fact_orders.count()} final fact table records\")\n",
    "print(\"\\n✓ Technologies Used:\")\n",
    "print(\"  • PySpark Structured Streaming\")\n",
    "print(\"  • Parquet Storage Format\")\n",
    "print(\"  • MongoDB Spark Connector\")\n",
    "print(\"  • MySQL JDBC Driver\")\n",
    "print(\"\\n✓ Analytics Queries: 5 business intelligence queries executed\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
